{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Encode\n",
    "First we are going to open and load the text from anna.txt file.\n",
    "Then we would like to convert it into integers for our network to use. \n",
    "Here I'm creating a couple dictionaries to convert the characters to and from integers. \n",
    "Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('anna.txt','r') as file:\n",
    "    text_data = file.read()\n",
    "# remove the duplicates and sort the characters in a list\n",
    "vocabulary_set = sorted(set(text_data))\n",
    "char_to_int = {char:i for i,char in enumerate(vocabulary_set)}\n",
    "int_to_char = dict(enumerate(vocabulary_set))\n",
    "encoded = np.array([char_to_int[char] for char in text_data], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of our text data: 1985223\n",
      "the first 50 characters of the text data:\n",
      "------------------------- characters from the book -------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every un'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('size of our text data:',len(text_data))\n",
    "print('the first 50 characters of the text data:')\n",
    "print('------------------------- characters from the book -------------------------------------------')\n",
    "text_data[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Same Encoded 50 Character -----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70], dtype=int32)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('-------------------------- Same Encoded 50 Character -----------------------------------------')\n",
    "encoded[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As you can see, character  n is decoded to  70\n"
     ]
    }
   ],
   "source": [
    "print('As you can see, character ',int_to_char[70], 'is decoded to ',char_to_int['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes in our data set: 83\n",
      "the first 50 characters of the sorted vocabulary set:\n",
      "{'7', '0', 'Q', '!', 'D', '*', 'I', 'T', '\"', '2', 'F', '&', '/', '%', 'H', '4', 'K', '3', 'U', 'B', 'N', '$', '.', ')', 'S', '-', '8', '9', \"'\", '@', 'G', 'O', ',', 'L', 'R', 'J', 'M', 'P', '1', '5', 'A', '\\n', '(', 'C', 'E', ' ', ';', '?', ':', '6'}\n"
     ]
    }
   ],
   "source": [
    "print('Number of Classes in our data set:', len(vocabulary_set))\n",
    "print('the first 50 characters of the sorted vocabulary set:')\n",
    "print(set(itertools.islice(vocabulary_set, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 17 unused characters in this text book which they are:\n",
      "{'=', '<', '#', '~', '{', '[', '\\x0b', '|', '\\x0c', '+', '\\t', ']', '^', '>', '}', '\\\\', '\\r'}\n"
     ]
    }
   ],
   "source": [
    "all_set = set(string.printable)\n",
    "# print(all_set)\n",
    "print('there are',len(all_set)-len(vocabulary_set),'unused characters in this text book which they are:')\n",
    "print(all_set - set(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Mini-Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mini batch generator is defined to yeild x and y in mini batches. \n",
    "We would like to cut the length of our data sequence to have K total batches of size N. The number of characters per batch is M. \n",
    "Sine we want to predict the next character in the sequence, y is simply the same as x, but shifted by one in our trainig set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is number of sequences = batch_size\n",
    "# M = num_steps\n",
    "# M * N = number of characters per batch\n",
    "# K is total number of batches\n",
    "def batch_generator(array, batch_size, num_steps):\n",
    "    # number of charachters per batch\n",
    "    n_char_per_batch = batch_size * num_steps\n",
    "    num_batches = len(array) // n_char_per_batch\n",
    "    array = array[:n_char_per_batch * num_batches]\n",
    "    array = np.reshape(array, (batch_size,-1))\n",
    "    # split array into batch_size of sequences\n",
    "    for n in range(num_steps):\n",
    "        x = array[:,n:n+num_steps]\n",
    "        y_temp = array[:,n+1:n+num_steps+1]\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "#         y[:, :-1], y[: ,-1] = x[:, 1:] , x[:, 0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n",
      "x\n",
      " (10, 50)\n",
      "\n",
      "y\n",
      " (10, 50)\n"
     ]
    }
   ],
   "source": [
    "batches = batch_generator(encoded, 10, 50)\n",
    "x,y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print('x\\n', x.shape)\n",
    "print('\\ny\\n', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer\n",
    "We need placeholders for inputs, targets and dropout layer keep_probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build one cell of LSTM and stack them up into as many as needed in one layer.\n",
    "We can have multiple hidden layers of LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build LSTM cell\n",
    "# num_layers : number of hidden layers (verical number of LSTM cells)\n",
    "# lstm_size : number of LAST cells horizontally in each hidden layer. This should be equal to number of steps that\n",
    "#we mini batch by.\n",
    "\n",
    "def LSTM_Cells(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    # one lstm cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    # one cell wrapped with dropout layer\n",
    "    dropped = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    # stack of lstm cells in the hidden layer\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([dropped]*num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders for x,y and keep_prob of dropout layers\n",
    "def input_generator(batch_size, num_steps):\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps],name='x')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps],name='y')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of RNN cells(hidden Layers) will be fully connected to output layer through softmax to produce predictions. So the size of this layer should be the same as size of our data set characters which is 83.\n",
    "So if we have N sequences of inputs, each with M steps, when they pass through L number of lstm cells in our hidden layer, the output will be size N . M . L. This is a 3D tensor object that we need to reshape in to a 2D tensor of shape (N . M) . L.\n",
    "Size of output of softmax layer is the same as size of logits or number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_layer(lstm_output, soft_in_size, soft_out_size):\n",
    "    # lstm_output : MN*L (2D tensor)\n",
    "    # soft_in_size : LSTM cells size (L)\n",
    "    # soft_out_size : number of classes\n",
    "    \n",
    "    output_sequence = tf.concat(lstm_output, axis=1)\n",
    "    softmax_input = tf.reshape(output_sequence, [-1, soft_in_size])\n",
    "    \n",
    "    # to not let the tensor get softmax weights confused with lstm weights:\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((soft_in_size, soft_out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(soft_out_size))\n",
    "    \n",
    "    logits = tf.matmul(softmax_input, softmax_w) + softmax_b\n",
    "    softmax_out = tf.nn.softmax(logits, name='prediction')\n",
    "    return softmax_out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss \n",
    "Targets are required to be one_hot_encoded before loss calculation. \n",
    "Cross-Entropy is used to calculate the loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(logits, targets, num_classes ):\n",
    "    # targets(y) : labels -> they need to be reshaped to logits shape and also one-hot encoded\n",
    "    one_hot_labels = tf.one_hot(targets, num_classes)\n",
    "    labels_reshaped = tf.reshape(one_hot_labels, logits.get_shape())\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Gradient Exploding Fix\n",
    "The optimizer will take in the loss and learning rate and use a threshold to clip the gradients, if they grow bigger than the threshold. This will avoid the problem of gradient exploding.\n",
    "Adamoptimizer has been used, which optionally can perform \"learning decay\" if required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer_unit(loss, learning_rate, clip_grad):\n",
    "    \n",
    "    tvar = tf.trainable_variables()\n",
    "    # tf.gradient calculates the symbolic gradients of loss with respect to weights at each time step\n",
    "    gradients, _ = tf.clip_by_global_norm(tf.gradients(loss, tvar), clip_grad)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate, name='Adam')\n",
    "    optimizer = train_op.apply_gradients(zip(gradients,tvar))\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Network\n",
    "Following is defined RNN class that initializes the one-hot-encoded input, lstm cells, output layer. It needs to use the last/final state of LSTM for the mini-batch, so the next batch continous the state from the previous batch.\n",
    "Then it will calculate the Loss and do the optimization.\n",
    "Out RNN network needs number of classes, batch size, number of steps per batch, lstm cell size, number of hidden layer, gradient threshold and learning rate as input arguments.\n",
    "*tf.nn.dynamic_rnn* will do the job of running the data through lstm cells for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_class:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50,\n",
    "                 num_hidden_layers=2, lstm_size=128, learning_rate=0.001,\n",
    "                 grad_clip=5, sampling=False):\n",
    "        \n",
    "        if sampling:\n",
    "             batch_size ,num_steps = 1 , 1\n",
    "        else: \n",
    "             batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        # reset the graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # RNN data flow        \n",
    "        #######input#######\n",
    "        self.inputs, self.targets, self.keep_prob = input_generator(batch_size, num_steps)\n",
    "        \n",
    "        #######LSTM cells######\n",
    "        LSTM_cells, self.initial_state = LSTM_Cells(lstm_size, num_hidden_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        # run inputs through the LSTM cells        \n",
    "        # one-hot encoded x input\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        LSTM_output, last_state = tf.nn.dynamic_rnn(LSTM_cells, x_one_hot, initial_state=self.initial_state)\n",
    "        # last state of previous output will be the fist state of next one\n",
    "        self.final_state = last_state\n",
    "        \n",
    "        #######output######\n",
    "        # softmax , predictions and logits\n",
    "        self.predictions, self.logits = output_layer(LSTM_output, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and Optimize\n",
    "        self.loss = loss_function(self.logits, self.targets, num_classes)\n",
    "        self.optimizer = optimizer_unit(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of Netwrok\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_hidden_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network\n",
    "To train the network we creat a model and pass it inputs and targets and run the optimizer.\n",
    "every often checkpoints are save with the following formats:\n",
    "i{iteration number}_l{# hidden layer units}.ckpt\n",
    "\n",
    "Steps taken to training the network are as followings:\n",
    "    \n",
    "  * initialize the epoch size, saving and printing frequencies  \n",
    "  * creat a saver instance  \n",
    "  * start the tf.session  \n",
    "  * globally initialize variablesin the session  \n",
    "  * load the checkpoint and resume training(optional)  \n",
    "  * for each epoch:\n",
    "    1. initialize the state of the model and loss\n",
    "    2. Go through each batch with \n",
    "          * A. preparing the feed(model inputs, model labels, model keep_prob and model initial_state)\n",
    "          * B. Calculate the loss and new state\n",
    "          * C. print time of training , epoch, step and loss\n",
    "          * D. Save the batch checkpoint\n",
    "    3. Save the epoch checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-289-c91e3e023b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbatch_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_counter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprinting_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "saving_freq = 200\n",
    "printing_freq = 50\n",
    "\n",
    "RNN_model = RNN_class(len(vocabulary_set), batch_size=batch_size, num_steps=num_steps,\n",
    "                 num_hidden_layers=num_hidden_layers, lstm_size=lstm_size, learning_rate=learning_rate,\n",
    "                 grad_clip=5, sampling=False)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     writer = tf.summary.FileWriter(\"./tmp/log/...\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    batch_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = 0\n",
    "        state = sess.run(RNN_model.initial_state)\n",
    "        for x,y in batch_generator(encoded, batch_size, num_steps):\n",
    "            start_time = time.time()\n",
    "            batch_counter += 1\n",
    "            feed = {RNN_model.inputs:x, RNN_model.targets:y, RNN_model.keep_prob:keep_prob, RNN_model.initial_state:state}\n",
    "            batch_loss, state, _ = sess.run([RNN_model.loss, RNN_model.final_state, RNN_model.optimizer],feed_dict=feed)\n",
    "\n",
    "            if batch_counter % printing_freq == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"{:.4f} sec/batch\".format(end_time - start_time),\n",
    "                      \"Epoch:{}/{}\".format(epoch+1, range(epoches)),\n",
    "                      \"batch number: {}\".format(batch_counter),\n",
    "                      \"training loss: {:.4f}\".format(batch_loss))\n",
    "\n",
    "            if batch_counter % saving_freq == 0:\n",
    "                saver.save(sess, \"checkpoint/iter_No{}_Layer_NO{}.ckpt\".format(batch_counter, lstm_size))\n",
    "\n",
    "        saver.save(sess, \"checkpoint/Epoch{}_Layer_NO{}.ckpt\".format(epoch, lstm_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
