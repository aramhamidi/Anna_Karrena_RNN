{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Encode\n",
    "First we are going to open and load the text from anna.txt file.\n",
    "Then we would like to convert it into integers for our network to use. \n",
    "Here I'm creating a couple dictionaries to convert the characters to and from integers. \n",
    "Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('anna.txt','r') as file:\n",
    "    text_data = file.read()\n",
    "# remove the duplicates and sort the characters in a list\n",
    "vocabulary_set = sorted(set(text_data))\n",
    "char_to_int = {char:i for i,char in enumerate(vocabulary_set)}\n",
    "int_to_char = dict(enumerate(vocabulary_set))\n",
    "encoded = np.array([char_to_int[char] for char in text_data], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of our text data: 1985223\n",
      "the first 50 characters of the text data:\n",
      "------------------------- characters from the book -------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every un'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('size of our text data:',len(text_data))\n",
    "print('the first 50 characters of the text data:')\n",
    "print('------------------------- characters from the book -------------------------------------------')\n",
    "text_data[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Same Encoded 50 Character -----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('-------------------------- Same Encoded 50 Character -----------------------------------------')\n",
    "encoded[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As you can see, character  n is decoded to  70\n"
     ]
    }
   ],
   "source": [
    "print('As you can see, character ',int_to_char[70], 'is decoded to ',char_to_int['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes in our data set: 83\n",
      "the first 50 characters of the sorted vocabulary set:\n",
      "{',', ' ', '\"', '-', '9', 'F', '(', 'I', '0', 'N', '8', 'G', '4', '?', '&', 'M', '%', '!', 'J', 'H', 'B', 'O', 'A', '*', ':', 'K', '$', 'T', '7', 'R', '@', '3', '/', '6', '5', ';', ')', 'S', \"'\", 'C', 'E', 'D', '2', 'L', 'U', 'Q', 'P', '\\n', '.', '1'}\n"
     ]
    }
   ],
   "source": [
    "print('Number of Classes in our data set:', len(vocabulary_set))\n",
    "print('the first 50 characters of the sorted vocabulary set:')\n",
    "print(set(itertools.islice(vocabulary_set, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 17 unused characters in this text book which they are:\n",
      "{'<', '\\r', ']', '\\\\', '[', '=', '#', '+', '~', '\\x0c', '}', '^', '|', '\\t', '\\x0b', '{', '>'}\n"
     ]
    }
   ],
   "source": [
    "all_set = set(string.printable)\n",
    "# print(all_set)\n",
    "print('there are',len(all_set)-len(vocabulary_set),'unused characters in this text book which they are:')\n",
    "print(all_set - set(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Mini-Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mini batch generator is defined to yeild x and y in mini batches. \n",
    "We would like to cut the length of our data sequence to have K total batches of size N. The number of characters per batch is M. \n",
    "Sine we want to predict the next character in the sequence, y is simply the same as x, but shifted by one in our trainig set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is number of sequences = batch_size\n",
    "# M = num_steps\n",
    "# M * N = number of characters per batch\n",
    "# K is total number of batches\n",
    "def batch_generator(array, batch_size, num_steps):\n",
    "    # number of charachters per batch\n",
    "    n_char_per_batch = batch_size * num_steps\n",
    "    num_batches = math.floor(len(array) / n_char_per_batch)\n",
    "    array = array[:n_char_per_batch * num_batches]\n",
    "    array = np.reshape(array, (batch_size,-1))\n",
    "    # split array into batch_size of sequences\n",
    "    for n in range(num_batches):\n",
    "        x = array[:,n:n+num_steps]\n",
    "#         y_temp = array[:,n+1:n+num_steps+1]\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "#         y[:,:y_temp.shape[1]] = y_temp\n",
    "        y[:, :-1], y[: ,-1] = x[:, 1:] , x[:, 0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n",
      "x\n",
      " (10, 50)\n",
      "\n",
      "y\n",
      " (10, 50)\n"
     ]
    }
   ],
   "source": [
    "batches = batch_generator(encoded, 10, 50)\n",
    "x,y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print('x\\n', x.shape)\n",
    "print('\\ny\\n', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders for x,y and keep_prob of dropout layers\n",
    "def input_generator(batch_size, num_steps):\n",
    "    inputs = tf.placeholder(tf.int32, shape=(batch_size, num_steps),name='x')\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size,num_steps),name='y')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build one cell of LSTM and stack them up into as many as needed in one layer.\n",
    "We can have multiple hidden layers of LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build LSTM cell\n",
    "# num_layers : number of hidden layers (verical number of LSTM cells)\n",
    "# lstm_size : number of LAST cells horizontally in each hidden layer. This should be equal to number of steps that\n",
    "#we mini batch by.\n",
    "\n",
    "def buil_LSTM_Cells(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    # one lstm cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    # one cell wrapped with dropout layer\n",
    "    dropped = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    # stack of lstm cells in the hidden layer\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([dropped]*num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.flout32)\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of RNN cells(hidden Layers) will be fully connected to output layer through softmax to produce predictions. So the size of this layer should be the same as size of our data set characters which is 83.\n",
    "So if we have N sequences of inputs, each with M steps, when they pass through L number of lstm cells in our hidden layer, the output will be size N . M . L. This is a 3D tensor object that we need to reshape in to a 2D tensor of shape (N . M) . L.\n",
    "Size of output of softmax layer is the same as size of logits or number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Gradient Exploding Fix\n",
    "The optimizer will take in the loss and learning rate and use a threshold to clip the gradients, if they grow bigger than the threshold. This will avoid the problem of gradient exploding.\n",
    "Adamoptimizer has been used, which optionally can perform \"learning decay\" if required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Network\n",
    "Following is defined RNN class that initializes the one-hot-encoded input, lstm cells, output layer. It needs to use the last/final state of LSTM for the mini-batch, so the next batch continous the state from the previous batch.\n",
    "Then it will calculate the Loss and do the optimization.\n",
    "Out RNN network needs number of classes, batch size, number of steps per batch, lstm cell size, number of hidden layer, gradient threshold and learning rate as input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of Netwrok\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network\n",
    "To train the network we creat a model and pass it inputs and targets and run the optimizer.\n",
    "every often checkpoints are save with the following formats:\n",
    "i{iteration number}_l{# hidden layer units}.ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
