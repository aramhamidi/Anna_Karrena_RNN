{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Encode\n",
    "First we are going to open and load the text from anna.txt file.\n",
    "Then we would like to convert it into integers for our network to use. \n",
    "Here I'm creating a couple dictionaries to convert the characters to and from integers. \n",
    "Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('anna.txt','r') as file:\n",
    "    text_data = file.read()\n",
    "# remove the duplicates and sort the characters in a list\n",
    "vocabulary_set = sorted(set(text_data))\n",
    "char_to_int = {char:i for i,char in enumerate(vocabulary_set)}\n",
    "int_to_char = dict(enumerate(vocabulary_set))\n",
    "encoded = np.array([char_to_int[char] for char in text_data], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of our text data: 1985223\n",
      "the first 50 characters of the text data:\n",
      "------------------------- characters from the book -------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every un'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('size of our text data:',len(text_data))\n",
    "print('the first 50 characters of the text data:')\n",
    "print('------------------------- characters from the book -------------------------------------------')\n",
    "text_data[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Same Encoded 50 Character -----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('-------------------------- Same Encoded 50 Character -----------------------------------------')\n",
    "encoded[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As you can see, character  n is decoded to  70\n"
     ]
    }
   ],
   "source": [
    "print('As you can see, character ',int_to_char[70], 'is decoded to ',char_to_int['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes in our data set: 83\n",
      "the first 50 characters of the sorted vocabulary set:\n",
      "{':', 'G', 'K', '(', ')', '8', 'T', 'Q', ',', '.', '2', ';', ' ', 'N', '9', '!', '%', 'A', '-', '?', '&', '@', 'R', 'S', '6', 'C', '/', '$', 'L', '7', 'P', '\"', '\\n', 'U', 'O', 'I', 'M', 'D', 'E', \"'\", '1', 'F', '*', 'H', '0', '3', '5', 'B', 'J', '4'}\n"
     ]
    }
   ],
   "source": [
    "print('Number of Classes in our data set:', len(vocabulary_set))\n",
    "print('the first 50 characters of the sorted vocabulary set:')\n",
    "print(set(itertools.islice(vocabulary_set, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 17 unused characters in this text book which they are:\n",
      "{'#', '\\\\', '{', '\\x0c', '}', '[', '\\x0b', '~', '+', ']', '^', '<', '>', '=', '\\t', '|', '\\r'}\n"
     ]
    }
   ],
   "source": [
    "all_set = set(string.printable)\n",
    "# print(all_set)\n",
    "print('there are',len(all_set)-len(vocabulary_set),'unused characters in this text book which they are:')\n",
    "print(all_set - set(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Mini-Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mini batch generator is defined to yeild x and y in mini batches. \n",
    "We would like to cut the length of our data sequence to have K total batches of size N. The number of characters per batch is M. \n",
    "Sine we want to predict the next character in the sequence, y is simply the same as x, but shifted by one in our trainig set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is number of sequences = batch_size\n",
    "# M = num_steps\n",
    "# M * N = number of characters per batch\n",
    "# K is total number of batches\n",
    "def batch_generator(array, batch_size, num_steps):\n",
    "    # number of charachters per batch\n",
    "    n_char_per_batch = batch_size * num_steps\n",
    "    num_batches = len(array) // n_char_per_batch\n",
    "    array = array[:n_char_per_batch * num_batches]\n",
    "    array = np.reshape(array, (batch_size,-1))\n",
    "    # split array into batch_size of sequences\n",
    "    for n in range(num_steps):\n",
    "        x = array[:,n:n+num_steps]\n",
    "        y_temp = array[:,n+1:n+num_steps+1]\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "#         y[:, :-1], y[: ,-1] = x[:, 1:] , x[:, 0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n",
      "x\n",
      " (10, 50)\n",
      "\n",
      "y\n",
      " (10, 50)\n"
     ]
    }
   ],
   "source": [
    "batches = batch_generator(encoded, 10, 50)\n",
    "x,y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print('x\\n', x.shape)\n",
    "print('\\ny\\n', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer\n",
    "We need placeholders for inputs, targets and dropout layer keep_probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for x,y and keep_prob of dropout layers\n",
    "def input_generator(batch_size, num_steps):\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps],name='x')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps],name='y')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build one cell of LSTM and stack them up into as many as needed in one layer.\n",
    "We can have multiple hidden layers of LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM cell\n",
    "# num_layers : number of hidden layers (verical number of LSTM cells)\n",
    "# lstm_size : number of LAST cells horizontally in each hidden layer. This should be equal to number of steps that\n",
    "#we mini batch by.\n",
    "\n",
    "def LSTM_Cells(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    # one lstm cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    # one cell wrapped with dropout layer\n",
    "    dropped = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    # stack of lstm cells in the hidden layer\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([dropped]*num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of RNN cells(hidden Layers) will be fully connected to output layer through softmax to produce predictions. So the size of this layer should be the same as size of our data set characters which is 83.\n",
    "So if we have N sequences of inputs, each with M steps, when they pass through L number of lstm cells in our hidden layer, the output will be size N . M . L. This is a 3D tensor object that we need to reshape in to a 2D tensor of shape (N . M) . L.\n",
    "Size of output of softmax layer is the same as size of logits or number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer(lstm_output, soft_in_size, soft_out_size):\n",
    "    # lstm_output : MN*L (2D tensor)\n",
    "    # soft_in_size : LSTM cells size (L)\n",
    "    # soft_out_size : number of classes\n",
    "    \n",
    "    output_sequence = tf.concat(lstm_output, axis=1)\n",
    "    softmax_input = tf.reshape(output_sequence, [-1, soft_in_size])\n",
    "    \n",
    "    # to not let the tensor get softmax weights confused with lstm weights:\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((soft_in_size, soft_out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(soft_out_size))\n",
    "    \n",
    "    logits = tf.matmul(softmax_input, softmax_w) + softmax_b\n",
    "    softmax_out = tf.nn.softmax(logits, name='prediction')\n",
    "    return softmax_out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss \n",
    "Targets are required to be one_hot_encoded before loss calculation. \n",
    "Cross-Entropy is used to calculate the loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, targets, num_classes ):\n",
    "    # targets(y) : labels -> they need to be reshaped to logits shape and also one-hot encoded\n",
    "    one_hot_labels = tf.one_hot(targets, num_classes)\n",
    "    labels_reshaped = tf.reshape(one_hot_labels, logits.get_shape())\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Gradient Exploding Fix\n",
    "The optimizer will take in the loss and learning rate and use a threshold to clip the gradients, if they grow bigger than the threshold. This will avoid the problem of gradient exploding.\n",
    "Adamoptimizer has been used, which optionally can perform \"learning decay\" if required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_unit(loss, learning_rate, clip_grad):\n",
    "    \n",
    "    tvar = tf.trainable_variables()\n",
    "    # tf.gradient calculates the symbolic gradients of loss with respect to weights at each time step\n",
    "    gradients, _ = tf.clip_by_global_norm(tf.gradients(loss, tvar), clip_grad)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate, name='Adam')\n",
    "    optimizer = train_op.apply_gradients(zip(gradients,tvar))\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Network\n",
    "Following is defined RNN class that initializes the one-hot-encoded input, lstm cells, output layer. It needs to use the last/final state of LSTM for the mini-batch, so the next batch continous the state from the previous batch.\n",
    "Then it will calculate the Loss and do the optimization.\n",
    "Out RNN network needs number of classes, batch size, number of steps per batch, lstm cell size, number of hidden layer, gradient threshold and learning rate as input arguments.\n",
    "*tf.nn.dynamic_rnn* will do the job of running the data through lstm cells for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_class:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50,\n",
    "                 num_hidden_layers=2, lstm_size=128, learning_rate=0.001,\n",
    "                 grad_clip=5, sampling=False):\n",
    "        \n",
    "        if sampling:\n",
    "             batch_size ,num_steps = 1 , 1\n",
    "        else: \n",
    "             batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        # reset the graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # RNN data flow        \n",
    "        #######input#######\n",
    "        self.inputs, self.targets, self.keep_prob = input_generator(batch_size, num_steps)\n",
    "        \n",
    "        #######LSTM cells######\n",
    "        LSTM_cells, self.initial_state = LSTM_Cells(lstm_size, num_hidden_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        # run inputs through the LSTM cells        \n",
    "        # one-hot encoded x input\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        LSTM_output, last_state = tf.nn.dynamic_rnn(LSTM_cells, x_one_hot, initial_state=self.initial_state)\n",
    "        # last state of previous output will be the fist state of next one\n",
    "        self.final_state = last_state\n",
    "        \n",
    "        #######output######\n",
    "        # softmax , predictions and logits\n",
    "        self.predictions, self.logits = output_layer(LSTM_output, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and Optimize\n",
    "        self.loss = loss_function(self.logits, self.targets, num_classes)\n",
    "        self.optimizer = optimizer_unit(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of Netwrok\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_hidden_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network\n",
    "To train the network we creat a model and pass it inputs and targets and run the optimizer.\n",
    "every often checkpoints are save with the following formats:\n",
    "i{iteration number}_l{# hidden layer units}.ckpt\n",
    "\n",
    "Steps taken to training the network are as followings:\n",
    "    \n",
    "  * initialize the epoch size, saving and printing frequencies  \n",
    "  * creat a saver instance  \n",
    "  * start the tf.session  \n",
    "  * globally initialize variablesin the session  \n",
    "  * load the checkpoint and resume training(optional)  \n",
    "  * for each epoch:\n",
    "    1. initialize the state of the model and loss\n",
    "    2. Go through each batch with \n",
    "          * A. preparing the feed(model inputs, model labels, model keep_prob and model initial_state)\n",
    "          * B. Calculate the loss and new state\n",
    "          * C. print time of training , epoch, step and loss\n",
    "          * D. Save the batch checkpoint\n",
    "    3. Save the epoch checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3059 sec/batch Epoch:1/20 batch number: 50 training loss: 3.1442\n",
      "0.3050 sec/batch Epoch:1/20 batch number: 100 training loss: 3.0602\n",
      "0.3052 sec/batch Epoch:2/20 batch number: 150 training loss: 2.8047\n",
      "0.3064 sec/batch Epoch:2/20 batch number: 200 training loss: 2.4395\n",
      "0.3058 sec/batch Epoch:3/20 batch number: 250 training loss: 2.2552\n",
      "0.3053 sec/batch Epoch:3/20 batch number: 300 training loss: 2.0733\n",
      "0.3054 sec/batch Epoch:4/20 batch number: 350 training loss: 1.8890\n",
      "0.3062 sec/batch Epoch:4/20 batch number: 400 training loss: 1.6908\n",
      "0.3071 sec/batch Epoch:5/20 batch number: 450 training loss: 1.3387\n",
      "0.3069 sec/batch Epoch:5/20 batch number: 500 training loss: 1.2225\n",
      "0.3066 sec/batch Epoch:6/20 batch number: 550 training loss: 0.8134\n",
      "0.3065 sec/batch Epoch:6/20 batch number: 600 training loss: 0.8896\n",
      "0.3058 sec/batch Epoch:7/20 batch number: 650 training loss: 0.5597\n",
      "0.3070 sec/batch Epoch:7/20 batch number: 700 training loss: 0.7140\n",
      "0.3077 sec/batch Epoch:8/20 batch number: 750 training loss: 0.4491\n",
      "0.3055 sec/batch Epoch:8/20 batch number: 800 training loss: 0.5887\n",
      "0.3058 sec/batch Epoch:9/20 batch number: 850 training loss: 0.3695\n",
      "0.3058 sec/batch Epoch:9/20 batch number: 900 training loss: 0.5021\n",
      "0.3100 sec/batch Epoch:10/20 batch number: 950 training loss: 0.3254\n",
      "0.3070 sec/batch Epoch:10/20 batch number: 1000 training loss: 0.4422\n",
      "0.3103 sec/batch Epoch:11/20 batch number: 1050 training loss: 0.2854\n",
      "0.3061 sec/batch Epoch:11/20 batch number: 1100 training loss: 0.3986\n",
      "0.3064 sec/batch Epoch:12/20 batch number: 1150 training loss: 0.2734\n",
      "0.3065 sec/batch Epoch:12/20 batch number: 1200 training loss: 0.3625\n",
      "0.3058 sec/batch Epoch:13/20 batch number: 1250 training loss: 0.2482\n",
      "0.3065 sec/batch Epoch:13/20 batch number: 1300 training loss: 0.3315\n",
      "0.3092 sec/batch Epoch:14/20 batch number: 1350 training loss: 0.2306\n",
      "0.3065 sec/batch Epoch:14/20 batch number: 1400 training loss: 0.2852\n",
      "0.3065 sec/batch Epoch:15/20 batch number: 1450 training loss: 0.2157\n",
      "0.3063 sec/batch Epoch:15/20 batch number: 1500 training loss: 0.2711\n",
      "0.3068 sec/batch Epoch:16/20 batch number: 1550 training loss: 0.1964\n",
      "0.3091 sec/batch Epoch:16/20 batch number: 1600 training loss: 0.2557\n",
      "0.3065 sec/batch Epoch:17/20 batch number: 1650 training loss: 0.1970\n",
      "0.3055 sec/batch Epoch:17/20 batch number: 1700 training loss: 0.2448\n",
      "0.3067 sec/batch Epoch:18/20 batch number: 1750 training loss: 0.1805\n",
      "0.3082 sec/batch Epoch:18/20 batch number: 1800 training loss: 0.2141\n",
      "0.3105 sec/batch Epoch:19/20 batch number: 1850 training loss: 0.1697\n",
      "0.3068 sec/batch Epoch:19/20 batch number: 1900 training loss: 0.2109\n",
      "0.3068 sec/batch Epoch:20/20 batch number: 1950 training loss: 0.1671\n",
      "0.3087 sec/batch Epoch:20/20 batch number: 2000 training loss: 0.2072\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "saving_freq = 200\n",
    "printing_freq = 50\n",
    "\n",
    "RNN_model = RNN_class(len(vocabulary_set), batch_size=batch_size, num_steps=num_steps,\n",
    "                 num_hidden_layers=num_hidden_layers, lstm_size=lstm_size, learning_rate=learning_rate,\n",
    "                 grad_clip=5, sampling=False)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     writer = tf.summary.FileWriter(\"./log/...\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    batch_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = 0\n",
    "        state = sess.run(RNN_model.initial_state)\n",
    "        for x,y in batch_generator(encoded, batch_size, num_steps):\n",
    "            start_time = time.time()\n",
    "            batch_counter += 1\n",
    "            feed = {RNN_model.inputs:x, RNN_model.targets:y, RNN_model.keep_prob:keep_prob, RNN_model.initial_state:state}\n",
    "            batch_loss, state, _ = sess.run([RNN_model.loss, RNN_model.final_state, RNN_model.optimizer],feed_dict=feed)\n",
    "\n",
    "            if batch_counter % printing_freq == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"{:.4f} sec/batch\".format(end_time - start_time),\n",
    "                      \"Epoch:{}/{}\".format(epoch+1, epochs),\n",
    "                      \"batch number: {}\".format(batch_counter),\n",
    "                      \"training loss: {:.4f}\".format(batch_loss))\n",
    "\n",
    "            if batch_counter % saving_freq == 0:\n",
    "                saver.save(sess, \"checkpoints/iter_No{}_Layer_NO{}.ckpt\".format(batch_counter, lstm_size))\n",
    "\n",
    "        saver.save(sess, \"checkpoints/Epoch{}_Layer_NO{}.ckpt\".format(epoch, lstm_size))\n",
    "#     writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating New Text \n",
    "Now from the trained network we can generate new text:\n",
    "1. We pass a random string as an input to the generator to start with.\n",
    "2. Then we creat our model\n",
    "3. Create a Saver instance\n",
    "4. Restore the session checkpoints\n",
    "5. initialize the model state\n",
    "6. for all the chars in the input string:\n",
    "    * Encode the character\n",
    "    * Run the model and append the predictions\n",
    "7. for all the samples:\n",
    "    * feed the predictions back to the netweork\n",
    "    * Run the mode and append the predictions to the sample list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(checkpoints, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [char for char in prime]\n",
    "    RNN_model = RNN_class(len(vocabulary_set), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoints)\n",
    "        init_state = sess.run(RNN_model.initial_state)\n",
    "        for char in prime:\n",
    "            x = np.zeros((1,1))   # x[[0]]\n",
    "            x[0,0]= char_to_int[char]\n",
    "            feed = {RNN_model.inputs:x, RNN_model.keep_prob:1.0, RNN_model.initial_state:init_state}\n",
    "            predics, new_state = sess.run([RNN_model.predictions, RNN_model.final_state],feed_dict=feed)\n",
    "        c = pick_top_n(predics, len(vocabulary_set))\n",
    "        samples.append(int_to_char[c])\n",
    "            \n",
    "        for sample in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {RNN_model.inputs:x, RNN_model.keep_prob:1.0,RNN_model.initial_state:new_state}\n",
    "            predics, new_state = sess.run([RNN_model.predictions, RNN_model.final_state], feed_dict=feed)\n",
    "            c = pick_top_n(predics, len(vocabulary_set))\n",
    "            samples.append(int_to_char[c])\n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/Epoch19_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch0_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No200_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch1_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch2_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No400_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch3_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch4_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No600_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch5_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch6_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No800_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch7_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch8_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No1000_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch9_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch10_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No1200_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch11_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch12_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No1400_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch13_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch14_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No1600_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch15_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch16_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No1800_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch17_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch18_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/iter_No2000_Layer_NO512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/Epoch19_Layer_NO512.ckpt\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fare had been his, and sut her lanct his\n",
      "marre, whith his faur curls, his blue\n",
      "eyes, and his plump, graceful little legs in tightly pulled-up\n",
      "stockings. Anna experienced almost physical pleaser. \n",
      "The flonk of the wat see andion, her he\n",
      "sunderethat whe stam of his hight wase\n",
      "diany, wele gelling thet weald vodchad and sten and shat leald not herpelss, and his plems, gfances all geling wath th chas and would have said the rownor to have to ho with the district\n",
      "authorities. Where one would have to write out sheaves of papers, herellake of him. Seizing the first pretext, she got\n",
      "up, and with her light, resolute step went for her album. The stairs up\n",
      "to her room came out on the landing of the gring oot out\n",
      "he mand of the lade tianome to he reant ly mad your ander.\n",
      "\n",
      "\"I son was ind abreat more now ald beched and love, with him, and the old wate the lough the light, stong what the last of wosd. En with a mease of his hand hew his\n",
      "he her her was precing of the barrit, and mose und with his unat the how out he d grant her and the pronow, stame ous\n",
      "of the warls of was on\n",
      "\n",
      "\"What you de Hat, sard on'r not eve you now her fang oun your leve, whice you can't give\n",
      "her; and the other sacrifices everything for you and asks for nothing.\n",
      "What are you to do? How corled her and the uabloss thim ng heated\n",
      "prosersous, and his haad mon unhander, he wis selight, was the slow he was going out, her said to a _soreet he\n",
      "plostecter to the atcomnane to him, and hid of ronch out of the war, he was glaving away her secret, and\n",
      "that her face, burning with the flush of shame, had betrayed her\n",
      "already.\n",
      "\n",
      "\"I tee have mere than whe\n",
      "did arvice and chisfert. She had been on the lookout for her,\n",
      "glancing at her watch every minute,, and, as so often happens, let slip\n",
      "just that minute when her visitor arrived, so that she would not have the strength to exchange it for the\n",
      "shameful position of a woman who has abandoned his touth and eancound of haviong sith, gut the sprite of that stering thite\n",
      "vouch ald as stor a \n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = text_generator(checkpoint, 2000, lstm_size, len(vocabulary_set), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generated by the RNN network doesn't make any sensce to us, but still it is pretty fascianting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
